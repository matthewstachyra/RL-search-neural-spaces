{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spec 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to NAS with RL. There, the actions were decided hyperparameters; here, actions are deciding which weights to go to. Rather than update weights (such as with backpropagation), we move to certain weights (i.e., dettached layers).\n",
    "\n",
    "The reward is the loss at the end\n",
    "\n",
    "Each detached layer retains the same data dimensionality, except for the dense layer\n",
    "\n",
    "Dense layers are a possible action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What information does the gradient give us? How can we use it?\n",
    "- The partial derivatives tell us the rate of change of the loss due to that weight. It is a vector of slopes. So then we have some interesting pieces of information:\n",
    "    - the weight \n",
    "    - the weight's rate of change\n",
    "    - the weight's importance (in terms of the partial derivative of the loss w.r.t. this weight)\n",
    "\n",
    "- A connection is that if I knew how a weight was changing, then I could know what to look for (here, what next layer we want because it has that weight at a desired value).\n",
    "\n",
    "When does the network stop?\n",
    "- One possibility is in the case of a CNN, it stops when it reaches the dense layer (i.e., when the RL agent chooses that for its action). \n",
    "\n",
    "What is the reward for the controller?\n",
    "- The loss at the end of the chain of attached layers.\n",
    "\n",
    "Using the gradient at each layer\n",
    "- Keep track of information about the weights by index\n",
    "- This is one benefit of having the same dimension for the layer/weights in each detached layer\n",
    "- The trend for a weight can indicate what change we want to make\n",
    "- But maybe this doesn't work because each change to the weight makes sense in terms of the context (i.e., the changes to all the other weights; e.g., a certain change in weight be unecessary if we make some other change to a different weight)\n",
    "\n",
    "What is my state space? Is it continuous or discrete?\n",
    "- If my states are the detached layers, then one state will have the same value regardless of where in the sequence of layers it and by which layers was it preceded - but of course that information (sequence and composition) are critically important to how the built network performs\n",
    "- Discrete: Each state is a network (i.e., the network after that action, which was the attachment of some layer)\n",
    "- Continuous: If each state is a network, how can approximate networks and use a continuous model?\n",
    "    - We can represent the network as a vector with something like [number_of_detached_layers, weights]\n",
    "    - The downside of this approach is there is a set number of layers (set architecture)\n",
    "\n",
    "Do the weights of individual detached layers get updated? When do they get updated?\n",
    "- TODO\n",
    "\n",
    "How is the value of the state (so that detached layer in this network, so far) getting updated?\n",
    "- TODO\n",
    "\n",
    "Is there an opportunity to use square matrices and the inverse property to engineer backwards a solution? Can we work back from some output to the matrices (weights) we need?\n",
    "- TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetachedLayer(nn.Module):\n",
    "    '''\n",
    "    Layer that will be chained with other layers by the\n",
    "    RL agent.\n",
    "    '''\n",
    "    def __init__(self, depth=1):\n",
    "        super.__init__()\n",
    "        self.depth = depth\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)  # final output has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # the '1' in flatten keeps the first dimension,\n",
    "        # which is the batch size 4\n",
    "        x = torch.flatten(x, 1) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('masters-thesis-3CL5olUq')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a4ad1d2f18a2ff8426cfdd9be91e5890d73ede4af27f55ec1abd481aac780cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
